<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Word Vectors | Intuition</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Word Vectors" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Judge a word by the company it keeps." />
<meta property="og:description" content="Judge a word by the company it keeps." />
<link rel="canonical" href="https://imtiazbdsgit.github.io/intuition/jupyter/2021/01/29/Word-Vectors-NLP.html" />
<meta property="og:url" content="https://imtiazbdsgit.github.io/intuition/jupyter/2021/01/29/Word-Vectors-NLP.html" />
<meta property="og:site_name" content="Intuition" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-29T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://imtiazbdsgit.github.io/intuition/jupyter/2021/01/29/Word-Vectors-NLP.html","@type":"BlogPosting","headline":"Word Vectors","dateModified":"2021-01-29T00:00:00-06:00","datePublished":"2021-01-29T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://imtiazbdsgit.github.io/intuition/jupyter/2021/01/29/Word-Vectors-NLP.html"},"description":"Judge a word by the company it keeps.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/intuition/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://imtiazbdsgit.github.io/intuition/feed.xml" title="Intuition" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','G-0996ZFVLMN','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/intuition/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/intuition/">Intuition</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/intuition/about/">About Me</a><a class="page-link" href="/intuition/search/">Search</a><a class="page-link" href="/intuition/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Word Vectors</h1><p class="page-description">Judge a word by the company it keeps.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-01-29T00:00:00-06:00" itemprop="datePublished">
        Jan 29, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      15 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/intuition/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/imtiazBDSgit/intuition/tree/master/_notebooks/2021-01-29-Word-Vectors-NLP.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/intuition/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/imtiazBDSgit/intuition/master?filepath=_notebooks%2F2021-01-29-Word-Vectors-NLP.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/intuition/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/imtiazBDSgit/intuition/blob/master/_notebooks/2021-01-29-Word-Vectors-NLP.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/intuition/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h4"><a href="#Overview:">Overview: </a></li>
<li class="toc-entry toc-h4"><a href="#Language-Models-(Unigrams,-Bigrams,-etc.)">Language Models (Unigrams, Bigrams, etc.) </a></li>
<li class="toc-entry toc-h4"><a href="#Continuos-Bag-of-Words">Continuos Bag of Words </a></li>
<li class="toc-entry toc-h4"><a href="#CBOW-FLOW">CBOW FLOW </a></li>
<li class="toc-entry toc-h4"><a href="#Skip-Gram">Skip Gram </a></li>
<li class="toc-entry toc-h4"><a href="#Skip-Gram-Flow">Skip Gram Flow </a></li>
<li class="toc-entry toc-h4"><a href="#Numerical-Demo">Numerical Demo </a></li>
<li class="toc-entry toc-h4"><a href="#Negative-Sampling">Negative Sampling </a></li>
<li class="toc-entry toc-h4"><a href="#Numerical-Demo-with-Negative-Sampling">Numerical Demo with Negative Sampling </a></li>
<li class="toc-entry toc-h4"><a href="#References">References </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-01-29-Word-Vectors-NLP.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Overview:">
<a class="anchor" href="#Overview:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overview:<a class="anchor-link" href="#Overview:"> </a>
</h4>
<p>The meaning of a word in language is determined mostly by the context. Treating words as atomic independent units would not provide a notion of similarity among them. In English language there are an estimated of 13 million tokens. 
It is trivial for humans for example to figure out that dogs and cat mean in the real world and they are similar to each other in multiple context like pets, neighborhood etc. But the question that has to be answered is how do we induce this notion of similarity between words?</p>
<p>Wittgenstein’s theories suggest, we can say that dogs are similar to cats because they often appear in the same contexts: it’s more likely to find dogs and cats related to words like <code>“house” and “garden”</code> than to words like <code>“sea” and “ocean”</code>. Taking this idea into consideration and thinking about semantics</p>
<p><img src="https://github.com/imtiazBDSgit/intuition/blob/master/_notebooks/images/semantics.jpg?raw=true" width="700" height="300"></p>
<p>Without a doubt Distributional semantics is going to do us a world of favor if we go in that direction.</p>
<p>For example:</p>
<ol>
<li>
<code>There are more cats in this room</code>. <b>(plural)</b>
</li>
<li>
<code>The best pet I ever had was a cat</code>. <b>(past tense)</b>
</li>
<li>
<code>A tom cat is louder</code>. <b>(gender)</b>
</li>
</ol>
<p>The way is to encode these words in some sort of multidimensional word space with the expectation that each dimension would catch some notion of semantics for example plurality, tense, gender etc. Now once we have that information in vector space per word we can use traditional similarity techniques like <b>cosine, Euclidean</b> etc. among different words.</p>
<p>The idea is to design a model where word vectors are the parameters of the model and to train the model on a certain objective which would at each iteration improve the parameters (word vectors).  At every iteration we run our model, evaluate the errors, and follow an update rule that has some notion of penalizing the model parameters that caused the error. Thus, we learn our word vectors. Yes you are thinking right <b>back propagation</b> is the word”.</p>
<p>There are two algorithms to achieve this:</p>
<ol>
<li>
<b>Continuous bag of words (CBOW)</b>: CBOW aims to predict a center word from the surrounding context in terms of word vectors.</li>
<li>
<b>Skip-gram</b>: Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word.</li>
</ol>
<p>There are two training methods to achieve the above:</p>
<ol>
<li><b>Negative Sampling</b></li>
<li><b>Hierarchical softmax</b></li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>Lets take a step back and examine our objective more carefully</code></p>
<p>We can try to create a model that will be able to learn one iteration at a time and eventually be able to encode the probability of a word given its context. Such type of a model in NLP world is generally called Language model.</p>
<h4 id="Language-Models-(Unigrams,-Bigrams,-etc.)">
<a class="anchor" href="#Language-Models-(Unigrams,-Bigrams,-etc.)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Language Models (Unigrams, Bigrams, etc.)<a class="anchor-link" href="#Language-Models-(Unigrams,-Bigrams,-etc.)"> </a>
</h4>
<p>First, we need to create such a model that will assign a probability to a sequence of tokens. Let us start with an example:</p>
<p><code>"The cat jumped over the puddle."</code></p>
<p>A good language model will give this sentence a high probability because this is a completely valid sentence, syntactically and semantically. Similarly, the sentence <code>"stock boil fish is toy"</code> should have a very low probability because it makes no sense.</p>
<p>Mathematically we can call this probability</p>
$$
P(w_1,w_2,w_3.....w_n)
$$<ol>
<li>Think that all words are independent then the joint probability will be basically product of individual probabilities of each word denoted by</li>
</ol>
$$
P(w_1,w_2,w_3.....w_n)=\prod_{i=1}^n P(w_i)
$$<ol>
<li>If we consider all the words independent we are missing out on the fact that the current word depends on the sequence of previous words. Moving further on that thought we can make the prediction of a word to depend upon the previous couple of words, that would mathematically be denoted by</li>
</ol>
$$
P(w_1,w_2,w_3.....w_n)=\prod_{i=1}^n P(w_i|w_{i-1})
$$<p>But the drawback with the above approaches we need to have a huge matrix which would save this pair wise word probabilities and it will require huge memory. Is there something that we can do without consuming this much memory?  Why not learn these patterns one iteration at a time?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Continuos-Bag-of-Words">
<a class="anchor" href="#Continuos-Bag-of-Words" aria-hidden="true"><span class="octicon octicon-link"></span></a>Continuos Bag of Words<a class="anchor-link" href="#Continuos-Bag-of-Words"> </a>
</h4>
<p><code>“The cat jumped over the puddle”</code></p>
<p>In this sentence we treat {the, cat, over, the, puddle} bag as context words and output as jumped. We represent our sentence with one hot word vectors
$X_C$ is our input context one hot vectors $Y_C$ is our output center word
We create two matrices, $V ϵ R^{n * |V|}$ and $U ϵ R^{|V| * n}$.</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/CBOW.png" alt="" style="max-width: 300px">
    
    
</figure>
</p>
<p><strong>n</strong> = embedding space dimension</p>
<p><strong>V</strong> is the input word matrix such that the ith column of V is the n-dimensional embedded vector for word $w_i$ when it is an input to this model.</p>
<p>We denote this n * 1 vector as $v_i$.</p>
<p>Similarly, <strong>U</strong> is the output word matrix. The jth row of U is an n-dimensional embedded vector for word $w_j$ when it is an output of the model. We denote this row of U as $u_j$.</p>
<p>Note that we do in fact learn two vectors for every word $w_i$ (i.e. input word vector $v_i$ and output word vector $u_i$).</p>
<h4 id="CBOW-FLOW">
<a class="anchor" href="#CBOW-FLOW" aria-hidden="true"><span class="octicon octicon-link"></span></a>CBOW FLOW<a class="anchor-link" href="#CBOW-FLOW"> </a>
</h4>
<ol>
<li>We generate our one hot word vectors for the input context of size
m : $(x^{(c-m)}, . . . , x^{(c-1)}, x^{(c+1)}, . . . , x^{(c+m)}\;\;  \epsilon \;\; R^{|V|})$</li>
<li>
<p>We get our embedded word vectors for the context</p>
<p>$(v^{c-m} =V^{(c-m)}, . . . , v^{(c-1)}=V^{(c-1)}, . . . , v^{(c+m)}=V^{(c+m)} \;\; \epsilon \;\; R^{n})$</p>
</li>
<li>
<p>Average these vectors to get  $\hat{v} = (v^{c-m}+.....+v^{c+1}+...+v^{c+m})/(2*m)  \;\;\epsilon \;\; R^{n}$</p>
</li>
<li>
<p>Generate a score vector $z = U\hat{v} \;\;\epsilon\;\; R^{|V|}$. As the dot product of similar vectors is higher, it will      push similar words close to each other in order to achieve a high score.</p>
</li>
</ol>
<ol>
<li>Turn the scores into probabilities $\hat{y}=softmax(z)\;\; \epsilon\;\; R^{|V|}$</li>
</ol>
<ol>
<li>
<p>We desire our probabilities generated, $\hat{y}\;\; \epsilon\;\; R^{|V|}$ , to match the true probabilities, $y\;\; \epsilon\;\; R^{|V|}$, which also happens to be the      one hot vector of the actual word</p>
</li>
<li>
<p>Here, we use a popular choice of distance/loss measure, cross entropy $H(\hat{y}, y)$.</p>
</li>
</ol>
<p>$\;\;\;\;\;\;\;\;\;\;H(\hat{y}, y)=-\sum_{j=1}^{|V|}y_j*log(\hat{y_j})$</p>
<ol>
<li>$y_j$ is just a one hot vector so $H(\hat{y}, y)=-y_j*log(\hat{y_j})$</li>
<li>We reduce this loss with respect to input and output matrices $W_{input}$ and $W_{output}$ using back propagation</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this formulation, c is the index where the correct word’s one
hot vector is 1. We can now consider the case where our prediction
was perfect and thus $\hat{y}_c = 1$.</p>
<p>We can then calculate $H(\hat{y},y) =1log(1) = 0$. Thus, for a perfect prediction, we face no penalty or
loss.</p>
<p>Now let us consider the opposite case where our prediction was very bad and thus $\hat{y_c} = 0.01$. As before, we can calculate our loss to
be $H(\hat{y}, y) = 1log(0.01)= 4.605$.</p>
<p>We can thus see that for probability
distributions, cross entropy provides us with a good measure of
distance</p>
<p>So now mapping this probability distribution to our word vectors we get</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/objective_cbow.PNG" alt="" style="max-width: 800px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Skip-Gram">
<a class="anchor" href="#Skip-Gram" aria-hidden="true"><span class="octicon octicon-link"></span></a>Skip Gram<a class="anchor-link" href="#Skip-Gram"> </a>
</h4>
<p>Another approach is to create a model such that given the center word "jumped", the model will be able to predict or generate the surrounding words <code>"The", "cat", "over", "the", "puddle". Here we call the word "jumped" the context.</code> We call this type of model a Skip-Gram model.</p>
<p>The setup is largely the same but we essentially swap our x and y i.e. x in the CBOW are
now y and vice-versa. The input one hot vector (center word) we will represent with an x (since there is only one). And the output vectors as $y^j$. We define V and U the same as in CBOW</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/./images/skipgram.PNG" alt="" style="max-width: 300px">
    
    
</figure>
</p>
<h4 id="Skip-Gram-Flow">
<a class="anchor" href="#Skip-Gram-Flow" aria-hidden="true"><span class="octicon octicon-link"></span></a>Skip Gram Flow<a class="anchor-link" href="#Skip-Gram-Flow"> </a>
</h4>
<p>We breakdown the way this model works in these 6 steps:</p>
<ol>
<li>We generate our one hot input vector $x \epsilon R^{|V|}$ of the center word.</li>
</ol>
<ol>
<li>We get our embedded word vector for the center word $v_c = Vx\;\;\epsilon \;R^n$</li>
</ol>
<ol>
<li>Generate a score vector $z = Uv_c$</li>
</ol>
<ol>
<li>Turn the score vector into probabilities,  $\hat{y} = softmax(z)$</li>
</ol>
<ol>
<li>Note that $\hat{y}_{c-m} ....\hat{y}_{c-1} \hat{y}_{c+1}.....\hat{y}_{c+m} \;$are the probabilities of observing each context word.</li>
</ol>
<ol>
<li>We desire our probability vector generated to match the true probabilities which is ${y}^{c-m} ....{y}^{c-1} {y}^{c+1}.....{y}^{c+m} \;$, the one hot vectors of the actual output.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The objective function we are trying to minimize assuming independence assumption. 
In other words, given the center word, all output words are completely independent.</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/objective_skipgram.PNG" alt="" style="max-width: 600px">
    
    
</figure>
</p>
<p>With this objective function, we can compute the gradients with
respect to the unknown parameters and at each iteration update
them via Stochastic Gradient Descent.</p>
<p>Note that</p>
<p>$J=-\sum_{j=0,j\neq m}^{2m} \log P(u_{c-m}|V_c)$</p>
<p>$J=-\sum_{j=0,j\neq m}^{2m} H(\hat{y}|y_{c-m+j})$</p>
<p>where $H(\hat{y},y_{c-m+j})\;$is the cross-entropy between the probability
vector $\hat{y}$ and the one-hot vector $y_{c-m+j}$.</p>
<p><strong>Matrix Flow Visualization of algorithm</strong></p>
<p>For illustration purpose, let's assume that the entire corpus is composed of the quote from the Game of Thrones, <strong>"The man who passes the sentence should swing the sword"</strong>. There are 10 words (T=10), and 8 unique words (V=8).</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/word2vec_skip-gram_matrix.png" alt="" style="max-width: 600px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Numerical-Demo">
<a class="anchor" href="#Numerical-Demo" aria-hidden="true"><span class="octicon octicon-link"></span></a>Numerical Demo<a class="anchor-link" href="#Numerical-Demo"> </a>
</h4>
<p>Forward Propagation: Computing hidden (projection) layer</p>
<p>Center word is "passes". Window size is size=1, making "the" and "who" context words. Hidden layer (h) is looked up from the input weight matrix.</p>
<p><strong>Forward Propogation</strong></p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/n1.png" alt="" style="max-width: 500px">
    
    
</figure>
</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/n2.png" alt="" style="max-width: 500px">
    
    
</figure>
</p>
<p><strong>Backward Propogation</strong>
<figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/n3.png" alt="" style="max-width: 500px">
    
    
</figure>
</p>
<p><strong>Computing gradient</strong> $\nabla W_{input}$</p>
<p>Gradients of input weight matrix $\frac{\partial J}{\partial W_{input}}$ are computed.</p>
<p>Unlike the input weight matrix $W_{input}$, all word vectors in the output weight matrix $W_{output}$ are updated.</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/n4.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/n5.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p>The <strong>updates</strong> to the input and output parameter matrices are done in below.</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/n6.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/n7.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Negative-Sampling">
<a class="anchor" href="#Negative-Sampling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Negative Sampling<a class="anchor-link" href="#Negative-Sampling"> </a>
</h4>
<p>There is an issue with the vanilla Skip-Gram — <strong>softmax is computationally very expensive</strong>, as it requires scanning through the entire output embedding matrix $W_{output}$ to compute the probability distribution of all V words, where V can be millions or more.</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/vanilla-skip-gram-complexity.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p>Instead we use an alternative called negative sampling with sigmoid function, which rephrases the problem into a set of independent binary classification task of algorithm complexity = O(K+1), where K typically has a range of [5,20].</p>
<p>Let's assume that the training corpus has 10,000 unique vocabs (V = 10000) and the hidden layer is 300-dimensional  (N = 300). This means that there are 3,000,000 neurons in the output weight matrix $W_{output}$ that need to be updated for each training sample and for the input weight matrix $W_{input}$, only 300 neurons are updated for each training sample. Since the size of the training corpus (T) is very large, updating 3M neurons for each training sample is unrealistic in terms of computational efficiency. Negative sampling addresses this issue by updating only a small fraction of the output weight neurons for each training sample.</p>
<p>In negative sampling, K negative samples are randomly drawn from a noise distribution. K is a hyper-parameter that can be empirically tuned, with a typical range of [5,20]. For each training sample (positive pair: w and cpos), you randomly draw K number of negative samples from a noise distribution Pn(w), and the model will update (K+1)×N neurons in the output weight matrix $W_{output}$. N is the dimension of the hidden layer (h), or the size of a word vector. +1 accounts for a positive sample.</p>
<p>With the above assumption, if you set K=9, the model will update (9+1)×300=3000 neurons, which is only 0.1% of the 3M neurons in Woutput. This is computationally much cheaper than the original Skip-Gram, and yet maintains a good quality of word vectors.</p>
<p>The below figure has 3-dimensional hidden layer (N=3), 11 vocabs (V=11), and 3 negative samples (K=3).</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_vs_skip.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>How does negative sampling work?</strong></p>
<p>With negative sampling, word vectors are no longer learned by predicting context words of a center word. Instead of using softmax to compute the V-dimensional probability distribution of observing an output word given an input word, $p(w_O|w_I)$, the model uses sigmoid function to learn to differentiate the actual context words (positive) from randomly drawn words (negative) from the noise distribution $P_n(w)$.</p>
<p>Assume that the center word is "regression".</p>
<p>It is likely to observe "regression" + {"logistic", "machine", "sigmoid", "supervised", "neural"}</p>
<p>pairs, but it is unlikely to observe "regression" + {"zebra", "pimples", "Gangnam-Style", "toothpaste", "idiot"}.</p>
<p>The model maximizes the probability p(D=1|w,cpos) of observing positive pairs, while minimizing the probability p(D=1|w,cneg) of observing negative pairs.</p>
<p>The idea is that if the model can distinguish between the likely (positive) pairs vs unlikely (negative) pairs, good word vectors will be learned.</p>
<p><strong>Negative sampling converts multi-classification task into binary-classification task.</strong></p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_binomial.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p><code>The new objective is to predict, for any given word-context pair (w,c), whether the word (c) is in the context window of the the center word (w) or not. Since the goal is to identify a given word as True (positive, D=1) or False (negative, D=0)</code></p>
<p>The probability of a word (c) appearing within the context of the center word (w) can be defined as</p>
<p>$p(D=1|w,c;\theta)=\frac{1}{1+exp(-\bar{c}_{output_{(j)}} \cdot w)}
\in \mathbb{R}^{1}$</p>
<p>c is the word you want to know whether it came from the context window or the noise distribution. w is the input (center) word, and θ is the weight matrix passed into the model.</p>
<p>Note that w is equivalent to the hidden layer h.</p>
<p>$\bar{c}_{output_{(j)}}$ is the word vector from the output weight matrix $W_{output}$</p>
<p>The equation ($1$) computes the probability that the given word (c) is a positive word (D=1). It only needs to be applied K+1 times instead of V times for every word in the vocabulary, because $\bar{c}_{output_{(j)}}$ comes from the concatenation of a true context word $c_{pos}$ and K negative words</p>
<p>$\bar{W}_{neg} = \{ \bar{c}_{neg, j}|j=1,\cdots,K \}$</p>
<p>$\bar{c}_{output{(j)}} \in \{\bar{c}_{pos}\} \cup \bar{W}_{neg}$</p>
<p>This probability is computed K+1 times to obtain a probability distribution of a true context word and K negative samples:</p>
<p>$\left[ \begin{array}{c} p(D=1|w,c_{pos}) \\ p(D=1|w,c_{neg, 1}) \\ p(D=1|w,c_{neg, 2}) \\ p(D=1|w,c_{neg, 3}) \\ \vdots \\ p(D=1|w,c_{neg, K}) \end{array} \right] 
= \frac{1}{1+exp(-(\{\bar{c}_{pos}\} \cup \bar{W}_{neg}) \cdot h)}\in \mathbb{R}^{K+1}$
Example of optimizing</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_opt_1.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p>By the time the output probability distribution is nearly one-hot-encoded as in iter=4 of the above figure, weight matrices θ are optimized and good word vectors are learned. This optimization is achieved by maximizing the dot product of positive pairs $\bar{c}_{pos}\cdot \bar{w}$ and minimizing the dot product of negative pairs $c_{neg}\cdot w$</p>
<p><strong>Noise Distribution</strong></p>
<p>$P_n(w) = \left(\frac{U(w)}{Z}\right)^{\alpha}$</p>
<p>Imagine a distribution of words based on how many times each word appeared in a corpus, denoted as $U(w)$ (this is called unigram distribution). For each word w, divide the number of times it appeared in a corpus by a normalization factor Z so that the distribution becomes a probability distribution of range [0,1) and sums up to 1. Raise the normalized distribution to the power of α so that the distribution is "smoothed-out". Then this becomes your noise distribution $P_n(w)$ — normalized frequency distribution of words raised to the power of α.</p>
<p>Raising the unigram distribution U(w) to the power of α has an effect of smoothing out the distribution. It attempts to combat the imbalance between common words and rare words by decreasing the probability of drawing common words, and increasing the probability drawing rare words.</p>
<p><em>Effect of raising power of unigram distribution U(w)</em></p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/noise_dist.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Numerical-Demo-with-Negative-Sampling">
<a class="anchor" href="#Numerical-Demo-with-Negative-Sampling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Numerical Demo with Negative Sampling<a class="anchor-link" href="#Numerical-Demo-with-Negative-Sampling"> </a>
</h4>
<p>Center (input) word is Ned, and window size is C=2, making Stark and is context words. Number of negative samples drawn from the noise distribution for each positive pair is K=3.</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_training.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p>The center word Ned will be observed in a pair with context words (postive) like Stark, because it is his last name. The same thing goes for is too, because is is a verb tense used to describe a singular object.</p>
<p>However, Ned most likely won't be observed in a pair with random words (negative) like pimples, zebra, donkey within the book. If the model can differentiate between positive pairs and negative pairs. good word vectors will be learned.</p>
<p><strong>Forward Propagation: Computing hidden (projection) layer</strong></p>
<p>Hidden layer (h) is looked up from $W_{input}$ by multiplying the one-hot-encoded input vector with the input weight matrix $W_{input}$.</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_hidden.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p><strong>Forward Propagation: Sigmoid output layer</strong></p>
<p>Output layer is a probability distribution of positive and negative words $c_{pos} \cup W_{neg}$</p>
<p>given a center word (w). It is computed with sigmoid $\sigma(x) = \frac{1}{1+exp(-x)}$</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_forward_2.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p><strong>Backward Propagation: Prediction Error</strong></p>
<p>The details about the prediction error is described above. Since our current positive word is <code>Stark</code>, $t_j=1$ for <code>Stark</code> and $t_j=$0  for other negative words (pimples, zebra, idiot).</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_backward_1.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p><strong>Backward Propagation: Computing $\nabla W_{input}$</strong></p>
<p>Gradients of input weight matrix $\frac{\partial J}{\partial W_{input}}$ are computed.
Just like vanilla Skip-Gram, only the word vector in the input weight matrix $W_{input}$ that corresponds to the input (center) word w is updated
<figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_backward_3.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p><strong>Backward Propagation: Computing $\nabla W_{output}$</strong></p>
<p>With negative sampling, only a fraction of word vectors in the output weight matrix $W_{output}$ is updated. Gradients for K+1 word vectors for positive and negative words in the Woutput are computed
<figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_backward_4.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p><strong>Backward Propagation: Updating Weight matrices</strong></p>
<p>Input and output weight matrices ($[W_{input} W_{output}]$) are updated
<figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_backward_5.png" alt="" style="max-width: 700px">
    
    
</figure>

<figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_backward_6.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Positive word pair: (Ned, is)</strong></p>
<p>The center word Ned has two context words: Stark and is. This means that we have two positive pairs = two updates to make. Since we already update the matrices ($[W_{input} W_{output}]$) using (Ned, Stark), we will use (Ned, is) to update weight matrices this time.</p>
<p>In negative sampling, we draw new K negative words for each positive pairs. Assume that we randomly drew coins, donkey, and machine as our negative words this time.</p>
<p><strong>Forward Propagation: Computing hidden (projection) layer</strong>
<figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_hidden_2.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p><strong>Forward Propagation: Sigmoid output layer</strong>
<figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_forward_2_2.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p><strong>Backward Propagation: Prediction Error</strong>
Our current positive word is Stark: $t_j=1$ for Stark and $t_j=0$ for other negative words (coins, donkey, and machine)
<figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_backward_1_2.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p><strong>Backward Propagation: Computing $\nabla W_{input}$</strong>
<figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_backward_2_2.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p><strong>Backward Propagation: Computing $\nabla W_{output}$</strong>
<figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_backward_3_2.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p><strong>Backward Propagation: Updating Weight matrices</strong>
<figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_backward_4_2.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>
<p><figure>
  
    <img class="docimage" src="/intuition/images/copied_from_nb/images/neg_backward_5_2.png" alt="" style="max-width: 700px">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="References">
<a class="anchor" href="#References" aria-hidden="true"><span class="octicon octicon-link"></span></a>References<a class="anchor-link" href="#References"> </a>
</h4>
<p><a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf">http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf</a>
<a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling">https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling</a></p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="imtiazBDSgit/intuition"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/intuition/jupyter/2021/01/29/Word-Vectors-NLP.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/intuition/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/intuition/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/intuition/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A blog for deep learning.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/imtiazBDSgit" title="imtiazBDSgit"><svg class="svg-icon grey"><use xlink:href="/intuition/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/imtiazkhan_ds" title="imtiazkhan_ds"><svg class="svg-icon grey"><use xlink:href="/intuition/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
