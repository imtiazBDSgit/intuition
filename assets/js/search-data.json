{
  
    
        "post0": {
            "title": "Word Vectors",
            "content": "Philosophy: Judge a word by the company it keeps. . Overview: . The meaning of a word in language is determined mostly by the context. Treating words as atomic independent units would not provide a notion of similarity among them. In English language there are an estimated of 13 million tokens. It is trivial for humans for example to figure out that dogs and cat mean in the real world and they are similar to each other in multiple context like pets, neighborhood etc. But the question that has to be answered is how do we induce this notion of similarity between words? . Wittgenstein’s theories suggest, we can say that dogs are similar to cats because they often appear in the same contexts: it’s more likely to find dogs and cats related to words like “house” and “garden” than to words like “sea” and “ocean”. Taking this idea into consideration and thinking about semantics . . Without a doubt Distributional semantics is going to do us a world of favor if we go in that direction. . For example: . There are more cats in this room. (plural) | The best pet I ever had was a cat. (past tense) | A tom cat is louder. (gender) | The way is to encode these words in some sort of multidimensional word space with the expectation that each dimension would catch some notion of semantics for example plurality, tense, gender etc. Now once we have that information in vector space per word we can use traditional similarity techniques like cosine, Euclidean etc. among different words. . The idea is to design a model where word vectors are the parameters of the model and to train the model on a certain objective which would at each iteration improve the parameters (word vectors). At every iteration we run our model, evaluate the errors, and follow an update rule that has some notion of penalizing the model parameters that caused the error. Thus, we learn our word vectors. Yes you are thinking right back propagation is the word”. . There are two algorithms to achieve this: . Continuous bag of words (CBOW): CBOW aims to predict a center word from the surrounding context in terms of word vectors. | Skip-gram: Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word. | There are two training methods to achieve the above: . Negative Sampling | Hierarchical softmax | Lets take a step back and examine our objective more carefully . We can try to create a model that will be able to learn one iteration at a time and eventually be able to encode the probability of a word given its context. Such type of a model in NLP world is generally called Language model. . Language Models (Unigrams, Bigrams, etc.) . First, we need to create such a model that will assign a probability to a sequence of tokens. Let us start with an example: . &quot;The cat jumped over the puddle.&quot; . A good language model will give this sentence a high probability because this is a completely valid sentence, syntactically and semantically. Similarly, the sentence &quot;stock boil fish is toy&quot; should have a very low probability because it makes no sense. . Mathematically we can call this probability . $$ P(w_1,w_2,w_3.....w_n) $$ Think that all words are independent then the joint probability will be basically product of individual probabilities of each word denoted by | $$ P(w_1,w_2,w_3.....w_n)= prod_{i=1}^n P(w_i) $$ If we consider all the words independent we are missing out on the fact that the current word depends on the sequence of previous words. Moving further on that thought we can make the prediction of a word to depend upon the previous couple of words, that would mathematically be denoted by | $$ P(w_1,w_2,w_3.....w_n)= prod_{i=1}^n P(w_i|w_{i-1}) $$But the drawback with the above approaches we need to have a huge matrix which would save this pair wise word probabilities and it will require huge memory. Is there something that we can do without consuming this much memory? Why not learn these patterns one iteration at a time? . Continuos Bag of Words . “The cat jumped over the puddle” . In this sentence we treat {the, cat, over, the, puddle} bag as context words and output as jumped. We represent our sentence with one hot word vectors $X_C$ is our input context one hot vectors $Y_C$ is our output center word We create two matrices, $V ϵ R^{n * |V|}$ and $U ϵ R^{|V| * n}$. . . n = embedding space dimension . V is the input word matrix such that the ith column of V is the n-dimensional embedded vector for word $w_i$ when it is an input to this model. . We denote this n * 1 vector as $v_i$. . Similarly, U is the output word matrix. The jth row of U is an n-dimensional embedded vector for word $w_j$ when it is an output of the model. We denote this row of U as $u_j$. . Note that we do in fact learn two vectors for every word $w_i$ (i.e. input word vector $v_i$ and output word vector $u_i$). . CBOW FLOW . We generate our one hot word vectors for the input context of size m : $(x^{(c-m)}, . . . , x^{(c-1)}, x^{(c+1)}, . . . , x^{(c+m)} ; ; epsilon ; ; R^{|V|})$ | We get our embedded word vectors for the context . $(v^{c-m} =V^{(c-m)}, . . . , v^{(c-1)}=V^{(c-1)}, . . . , v^{(c+m)}=V^{(c+m)} ; ; epsilon ; ; R^{n})$ . | Average these vectors to get $ hat{v} = (v^{c-m}+.....+v^{c+1}+...+v^{c+m})/(2*m) ; ; epsilon ; ; R^{n}$ . | Generate a score vector $z = U hat{v} ; ; epsilon ; ; R^{|V|}$. As the dot product of similar vectors is higher, it will push similar words close to each other in order to achieve a high score. . | Turn the scores into probabilities $ hat{y}=softmax(z) ; ; epsilon ; ; R^{|V|}$ | We desire our probabilities generated, $ hat{y} ; ; epsilon ; ; R^{|V|}$ , to match the true probabilities, $y ; ; epsilon ; ; R^{|V|}$, which also happens to be the one hot vector of the actual word . | Here, we use a popular choice of distance/loss measure, cross entropy $H( hat{y}, y)$. . | $ ; ; ; ; ; ; ; ; ; ;H( hat{y}, y)=- sum_{j=1}^{|V|}y_j*log( hat{y_j})$ . $y_j$ is just a one hot vector so $H( hat{y}, y)=-y_j*log( hat{y_j})$ | We reduce this loss with respect to input and output matrices $W_{input}$ and $W_{output}$ using back propagation | In this formulation, c is the index where the correct word’s one hot vector is 1. We can now consider the case where our prediction was perfect and thus $ hat{y}_c = 1$. . We can then calculate $H( hat{y},y) =1log(1) = 0$. Thus, for a perfect prediction, we face no penalty or loss. . Now let us consider the opposite case where our prediction was very bad and thus $ hat{y_c} = 0.01$. As before, we can calculate our loss to be $H( hat{y}, y) = 1log(0.01)= 4.605$. . We can thus see that for probability distributions, cross entropy provides us with a good measure of distance . So now mapping this probability distribution to our word vectors we get . . Skip Gram . Another approach is to create a model such that given the center word &quot;jumped&quot;, the model will be able to predict or generate the surrounding words &quot;The&quot;, &quot;cat&quot;, &quot;over&quot;, &quot;the&quot;, &quot;puddle&quot;. Here we call the word &quot;jumped&quot; the context. We call this type of model a Skip-Gram model. . The setup is largely the same but we essentially swap our x and y i.e. x in the CBOW are now y and vice-versa. The input one hot vector (center word) we will represent with an x (since there is only one). And the output vectors as $y^j$. We define V and U the same as in CBOW . . We breakdown the way this model works in these 6 steps: . We generate our one hot input vector $x epsilon R^{|V|}$ of the center word. | We get our embedded word vector for the center word $v_c = Vx ; ; epsilon ;R^n$ | Generate a score vector $z = Uv_c$ | Turn the score vector into probabilities, $ hat{y} = softmax(z)$ | Note that $ hat{y}_{c-m} .... hat{y}_{c-1} hat{y}_{c+1}..... hat{y}_{c+m} ;$are the probabilities of observing each context word. | We desire our probability vector generated to match the true probabilities which is ${y}^{c-m} ....{y}^{c-1} {y}^{c+1}.....{y}^{c+m} ;$, the one hot vectors of the actual output. | The objective function we are trying to minimize assuming independence assumption. In other words, given the center word, all output words are completely independent. . . With this objective function, we can compute the gradients with respect to the unknown parameters and at each iteration update them via Stochastic Gradient Descent. . Note that . $J=- sum_{j=0,j neq m}^{2m} log P(u_{c-m}|V_c)$ . $J=- sum_{j=0,j neq m}^{2m} H( hat{y}|y_{c-m+j})$ . where $H( hat{y},y_{c-m+j}) ;$is the cross-entropy between the probability vector $ hat{y}$ and the one-hot vector $y_{c-m+j}$. . Matrix Flow Visualization of algorithm . For illustration purpose, let&#39;s assume that the entire corpus is composed of the quote from the Game of Thrones, &quot;The man who passes the sentence should swing the sword&quot;. There are 10 words (T=10), and 8 unique words (V=8). . . Numerical Demonstration of the end to end process . Forward Propagation: Computing hidden (projection) layer . Center word is &quot;passes&quot;. Window size is size=1, making &quot;the&quot; and &quot;who&quot; context words. Hidden layer (h) is looked up from the input weight matrix. . Forward Propogation . . . Backward Propogation . Computing gradient $ nabla W_{input}$ . Gradients of input weight matrix $ frac{ partial J}{ partial W_{input}}$ are computed. . Unlike the input weight matrix $W_{input}$, all word vectors in the output weight matrix $W_{output}$ are updated. . . . The updates to the input and output parameter matrices are done in below. . . . Negative Sampling . There is an issue with the vanilla Skip-Gram — softmax is computationally very expensive, as it requires scanning through the entire output embedding matrix $W_{output}$ to compute the probability distribution of all V words, where V can be millions or more. . . Instead we use an alternative called negative sampling with sigmoid function, which rephrases the problem into a set of independent binary classification task of algorithm complexity = O(K+1), where K typically has a range of [5,20]. . Let&#39;s assume that the training corpus has 10,000 unique vocabs (V = 10000) and the hidden layer is 300-dimensional (N = 300). This means that there are 3,000,000 neurons in the output weight matrix $W_{output}$ that need to be updated for each training sample and for the input weight matrix $W_{input}$, only 300 neurons are updated for each training sample. Since the size of the training corpus (T) is very large, updating 3M neurons for each training sample is unrealistic in terms of computational efficiency. Negative sampling addresses this issue by updating only a small fraction of the output weight neurons for each training sample. . In negative sampling, K negative samples are randomly drawn from a noise distribution. K is a hyper-parameter that can be empirically tuned, with a typical range of [5,20]. For each training sample (positive pair: w and cpos), you randomly draw K number of negative samples from a noise distribution Pn(w), and the model will update (K+1)×N neurons in the output weight matrix $W_{output}$. N is the dimension of the hidden layer (h), or the size of a word vector. +1 accounts for a positive sample. . With the above assumption, if you set K=9, the model will update (9+1)×300=3000 neurons, which is only 0.1% of the 3M neurons in Woutput. This is computationally much cheaper than the original Skip-Gram, and yet maintains a good quality of word vectors. . The below figure has 3-dimensional hidden layer (N=3), 11 vocabs (V=11), and 3 negative samples (K=3). . . How does negative sampling work? . With negative sampling, word vectors are no longer learned by predicting context words of a center word. Instead of using softmax to compute the V-dimensional probability distribution of observing an output word given an input word, $p(w_O|w_I)$, the model uses sigmoid function to learn to differentiate the actual context words (positive) from randomly drawn words (negative) from the noise distribution $P_n(w)$. . Assume that the center word is &quot;regression&quot;. . It is likely to observe &quot;regression&quot; + {&quot;logistic&quot;, &quot;machine&quot;, &quot;sigmoid&quot;, &quot;supervised&quot;, &quot;neural&quot;} . pairs, but it is unlikely to observe &quot;regression&quot; + {&quot;zebra&quot;, &quot;pimples&quot;, &quot;Gangnam-Style&quot;, &quot;toothpaste&quot;, &quot;idiot&quot;}. . The model maximizes the probability p(D=1|w,cpos) of observing positive pairs, while minimizing the probability p(D=1|w,cneg) of observing negative pairs. . The idea is that if the model can distinguish between the likely (positive) pairs vs unlikely (negative) pairs, good word vectors will be learned. . Negative sampling converts multi-classification task into binary-classification task. . . The new objective is to predict, for any given word-context pair (w,c), whether the word (c) is in the context window of the the center word (w) or not. Since the goal is to identify a given word as True (positive, D=1) or False (negative, D=0) . The probability of a word (c) appearing within the context of the center word (w) can be defined as . $p(D=1|w,c; theta)= frac{1}{1+exp(- bar{c}_{output_{(j)}} cdot w)} in mathbb{R}^{1} tag{1}$ . c is the word you want to know whether it came from the context window or the noise distribution. w is the input (center) word, and θ is the weight matrix passed into the model. . Note that w is equivalent to the hidden layer h. . $ bar{c}_{output_{(j)}}$ is the word vector from the output weight matrix $W_{output}$ . The equation ($1$) computes the probability that the given word (c) is a positive word (D=1). It only needs to be applied K+1 times instead of V times for every word in the vocabulary, because $ bar{c}_{output_{(j)}}$ comes from the concatenation of a true context word $c_{pos}$ and K negative words . $ bar{W}_{neg} = { bar{c}_{neg, j}|j=1, cdots,K }$ . $ bar{c}_{output{(j)}} in { bar{c}_{pos} } cup bar{W}_{neg} tag{2}$ . This probability is computed K+1 times to obtain a probability distribution of a true context word and K negative samples: . $ left[ begin{array}{c} p(D=1|w,c_{pos}) p(D=1|w,c_{neg, 1}) p(D=1|w,c_{neg, 2}) p(D=1|w,c_{neg, 3}) vdots p(D=1|w,c_{neg, K}) end{array} right] = frac{1}{1+exp(-( { bar{c}_{pos} } cup bar{W}_{neg}) cdot h)} in mathbb{R}^{K+1} tag{3}$ Example of optimizing . . By the time the output probability distribution is nearly one-hot-encoded as in iter=4 of the above figure, weight matrices θ are optimized and good word vectors are learned. This optimization is achieved by maximizing the dot product of positive pairs $ bar{c}_{pos} cdot bar{w}$ and minimizing the dot product of negative pairs $c_{neg} cdot w$ . Noise Distribution . $P_n(w) = left( frac{U(w)}{Z} right)^{ alpha}$ . Imagine a distribution of words based on how many times each word appeared in a corpus, denoted as $U(w)$ (this is called unigram distribution). For each word w, divide the number of times it appeared in a corpus by a normalization factor Z so that the distribution becomes a probability distribution of range [0,1) and sums up to 1. Raise the normalized distribution to the power of α so that the distribution is &quot;smoothed-out&quot;. Then this becomes your noise distribution $P_n(w)$ — normalized frequency distribution of words raised to the power of α. . Raising the unigram distribution U(w) to the power of α has an effect of smoothing out the distribution. It attempts to combat the imbalance between common words and rare words by decreasing the probability of drawing common words, and increasing the probability drawing rare words. . Effect of raising power of unigram distribution U(w) . . Numerical Demo . Center (input) word is Ned, and window size is C=2, making Stark and is context words. Number of negative samples drawn from the noise distribution for each positive pair is K=3. . . The center word Ned will be observed in a pair with context words (postive) like Stark, because it is his last name. The same thing goes for is too, because is is a verb tense used to describe a singular object. . However, Ned most likely won&#39;t be observed in a pair with random words (negative) like pimples, zebra, donkey within the book. If the model can differentiate between positive pairs and negative pairs. good word vectors will be learned. . Forward Propagation: Computing hidden (projection) layer . Hidden layer (h) is looked up from $W_{input}$ by multiplying the one-hot-encoded input vector with the input weight matrix $W_{input}$. . . Forward Propagation: Sigmoid output layer . Output layer is a probability distribution of positive and negative words $c_{pos} cup W_{neg}$ . given a center word (w). It is computed with sigmoid $ sigma(x) = frac{1}{1+exp(-x)}$ . . Backward Propagation: Prediction Error . The details about the prediction error is described above. Since our current positive word is Stark, $t_j=1$ for Stark and $t_j=$0 for other negative words (pimples, zebra, idiot). . . Backward Propagation: Computing $ nabla W_{input}$ . Gradients of input weight matrix $ frac{ partial J}{ partial W_{input}}$ are computed. Just like vanilla Skip-Gram, only the word vector in the input weight matrix $W_{input}$ that corresponds to the input (center) word w is updated . Backward Propagation: Computing $ nabla W_{output}$ . With negative sampling, only a fraction of word vectors in the output weight matrix $W_{output}$ is updated. Gradients for K+1 word vectors for positive and negative words in the Woutput are computed . Backward Propagation: Updating Weight matrices . Input and output weight matrices ($[W_{input} W_{output}]$) are updated . Positive word pair: (Ned, is) . The center word Ned has two context words: Stark and is. This means that we have two positive pairs = two updates to make. Since we already update the matrices ($[W_{input} W_{output}]$) using (Ned, Stark), we will use (Ned, is) to update weight matrices this time. . In negative sampling, we draw new K negative words for each positive pairs. Assume that we randomly drew coins, donkey, and machine as our negative words this time. . Forward Propagation: Computing hidden (projection) layer . Forward Propagation: Sigmoid output layer . Backward Propagation: Prediction Error Our current positive word is Stark: $t_j=1$ for Stark and $t_j=0$ for other negative words (coins, donkey, and machine) . Backward Propagation: Computing $ nabla W_{input}$ . Backward Propagation: Computing $ nabla W_{output}$ . Backward Propagation: Updating Weight matrices . . References . http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling .",
            "url": "https://imtiazbdsgit.github.io/intuition/jupyter/2021/01/29/Word-Vectors-NLP.html",
            "relUrl": "/jupyter/2021/01/29/Word-Vectors-NLP.html",
            "date": " • Jan 29, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://imtiazbdsgit.github.io/intuition/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://imtiazbdsgit.github.io/intuition/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://imtiazbdsgit.github.io/intuition/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://imtiazbdsgit.github.io/intuition/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}