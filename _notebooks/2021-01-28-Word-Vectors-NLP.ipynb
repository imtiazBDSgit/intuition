{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suspended-relaxation",
   "metadata": {},
   "source": [
    "# WORD VECTORS.\n"
    "### `Philosophy: Judge a word by the company it keeps.`\n",
    "\n",
    "#### Overview: \n",
    "\n",
    "The meaning of a word in language is determined mostly by the context. Treating words as atomic independent units would not provide a notion of similarity among them. In English language there are an estimated of 13 million tokens. \n",
    "It is trivial for humans for example to figure out that dogs and cat mean in the real world and they are similar to each other in multiple context like pets, neighborhood etc. But the question that has to be answered is how do we induce this notion of similarity between words?\n",
    "\n",
    "Wittgenstein’s theories suggest, we can say that dogs are similar to cats because they often appear in the same contexts: it’s more likely to find dogs and cats related to words like `“house” and “garden”` than to words like `“sea” and “ocean”`. Taking this idea into consideration and thinking about semantics\n",
    "\n",
    " <img src=\"images/semantics.PNG\" width=\"700\" height=\"300\"> \n",
    " \n",
    " Without a doubt Distributional semantics is going to do us a world of favor if we go in that direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-bradley",
   "metadata": {},
   "source": [
    "For example:\n",
    "\n",
    "1.\t`There are more cats in this room`. <b>(plural)</b>\n",
    "2.\t`The best pet I ever had was a cat`. <b>(past tense)</b>\n",
    "3.\t`A tom cat is louder`. <b>(gender)</b>\n",
    "\n",
    "The way is to encode these words in some sort of multidimensional word space with the expectation that each dimension would catch some notion of semantics for example plurality, tense, gender etc. Now once we have that information in vector space per word we can use traditional similarity techniques like <b>cosine, Euclidean</b> etc. among different words.\n",
    "\n",
    "The idea is to design a model where word vectors are the parameters of the model and to train the model on a certain objective which would at each iteration improve the parameters (word vectors).  At every iteration we run our model, evaluate the errors, and follow an update rule that has some notion of penalizing the model parameters that caused the error. Thus, we learn our word vectors. Yes you are thinking right <b>back propagation</b> is the word”.\n",
    "\n",
    "\n",
    "There are two algorithms to achieve this:\n",
    "\n",
    "1.\t<b>Continuous bag of words (CBOW)</b>: CBOW aims to predict a center word from the surrounding context in terms of word vectors.\n",
    "2.\t<b>Skip-gram</b>: Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word.\n",
    "\n",
    "There are two training methods to achieve the above:\n",
    "1.  <b>Negative Sampling</b>\n",
    "2.\t<b>Hierarchical softmax</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-plumbing",
   "metadata": {},
   "source": [
    "`Lets take a step back and examine our objective more carefully`\n",
    "\n",
    "We can try to create a model that will be able to learn one iteration at a time and eventually be able to encode the probability of a word given its context. Such type of a model in NLP world is generally called Language model.\n",
    "\n",
    "##### Language Models (Unigrams, Bigrams, etc.)\n",
    "\n",
    "First, we need to create such a model that will assign a probability to a sequence of tokens. Let us start with an example:\n",
    "\n",
    "`\"The cat jumped over the puddle.\"`\n",
    "\n",
    "A good language model will give this sentence a high probability because this is a completely valid sentence, syntactically and semantically. Similarly, the sentence `\"stock boil fish is toy\"` should have a very low probability because it makes no sense.\n",
    "\n",
    "Mathematically we can call this probability\n",
    "\n",
    "$$\n",
    "P(w_1,w_2,w_3.....w_n)\n",
    "$$\n",
    "\n",
    "1.\tThink that all words are independent then the joint probability will be basically product of individual probabilities of each word denoted by\n",
    "\n",
    "$$\n",
    "P(w_1,w_2,w_3.....w_n)=\\prod_{i=1}^n P(w_i)\n",
    "$$\n",
    "\n",
    "2.\tIf we consider all the words independent we are missing out on the fact that the current word depends on the sequence of previous words. Moving further on that thought we can make the prediction of a word to depend upon the previous couple of words, that would mathematically be denoted by\n",
    "\n",
    "$$\n",
    "P(w_1,w_2,w_3.....w_n)=\\prod_{i=1}^n P(w_i|w_{i-1})\n",
    "$$\n",
    "\n",
    "But the drawback with the above approaches we need to have a huge matrix which would save this pair wise word probabilities and it will require huge memory. Is there something that we can do without consuming this much memory?  Why not learn these patterns one iteration at a time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-vertex",
   "metadata": {},
   "source": [
    "### Continuos Bag of Words\n",
    "\n",
    "`“The cat jumped over the puddle”` \n",
    "\n",
    "In this sentence we treat {the, cat, over, the, puddle} bag as context words and output as jumped. We represent our sentence with one hot word vectors\n",
    "$X_C$ is our input context one hot vectors $Y_C$ is our output center word\n",
    "We create two matrices, $V ϵ R^{n * |V|}$ and $U ϵ R^{|V| * n}$. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/CBOW.png\" width=\"300\" height=\"900\" align='right'> \n",
    "\n",
    "**n** = embedding space dimension\n",
    "\n",
    "**V** is the input word matrix such that the ith column of V is the n-dimensional embedded vector for word $w_i$ when it is an input to this model. \n",
    "\n",
    "We denote this n * 1 vector as $v_i$. \n",
    "\n",
    "\n",
    "\n",
    "Similarly, **U** is the output word matrix. The jth row of U is an n-dimensional embedded vector for word $w_j$ when it is an output of the model. We denote this row of U as $u_j$.\n",
    "\n",
    "Note that we do in fact learn two vectors for every word $w_i$ (i.e. input word vector $v_i$ and output word vector $u_i$). \n",
    "\n",
    "##### CBOW FLOW\n",
    "\n",
    "1. We generate our one hot word vectors for the input context of size\n",
    "m : $(x^{(c-m)}, . . . , x^{(c-1)}, x^{(c+1)}, . . . , x^{(c+m)}\\;\\;  \\epsilon \\;\\; R^{|V|})$\n",
    "2. We get our embedded word vectors for the context \n",
    "\n",
    "    $(v^{c-m} =V^{(c-m)}, . . . , v^{(c-1)}=V^{(c-1)}, . . . , v^{(c+m)}=V^{(c+m)} \\;\\; \\epsilon \\;\\; R^{n})$\n",
    "    \n",
    "3. Average these vectors to get  $\\hat{v} = (v^{c-m}+.....+v^{c+1}+...+v^{c+m})/(2*m)  \\;\\;\\epsilon \\;\\; R^{n}$\n",
    "\n",
    "4. Generate a score vector $z = U\\hat{v} \\;\\;\\epsilon\\;\\; R^{|V|}$. As the dot product of similar vectors is higher, it will      push similar words close to each other in order to achieve a high score.\n",
    "\n",
    "\n",
    "5. Turn the scores into probabilities $\\hat{y}=softmax(z)\\;\\; \\epsilon\\;\\; R^{|V|}$\n",
    "\n",
    "\n",
    "6. We desire our probabilities generated, $\\hat{y}\\;\\; \\epsilon\\;\\; R^{|V|}$ , to match the true probabilities, $y\\;\\; \\epsilon\\;\\; R^{|V|}$, which also happens to be the      one hot vector of the actual word\n",
    "\n",
    "7. Here, we use a popular choice of distance/loss measure, cross entropy $H(\\hat{y}, y)$.\n",
    "\n",
    "\n",
    " $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;H(\\hat{y}, y)=-\\sum_{j=1}^{|V|}y_j*log(\\hat{y_j})$\n",
    " \n",
    "8. $y_j$ is just a one hot vector so $H(\\hat{y}, y)=-y_j*log(\\hat{y_j})$\n",
    "9. We reduce this loss with respect to input and output matrices $W_{input}$ and $W_{output}$ using back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-center",
   "metadata": {},
   "source": [
    "In this formulation, c is the index where the correct word’s one\n",
    "hot vector is 1. We can now consider the case where our prediction\n",
    "was perfect and thus $\\hat{y}_c = 1$. \n",
    "\n",
    "We can then calculate $H(\\hat{y},y) =1log(1) = 0$. Thus, for a perfect prediction, we face no penalty or\n",
    "loss. \n",
    "\n",
    "Now let us consider the opposite case where our prediction was very bad and thus $\\hat{y_c} = 0.01$. As before, we can calculate our loss to\n",
    "be $H(\\hat{y}, y) = 1log(0.01)= 4.605$. \n",
    "\n",
    "We can thus see that for probability\n",
    "distributions, cross entropy provides us with a good measure of\n",
    "distance\n",
    "\n",
    "So now mapping this probability distribution to our word vectors we get\n",
    "\n",
    "<img src=\"images/objective_cbow.PNG\" width=\"800\" height=\"200\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-musical",
   "metadata": {},
   "source": [
    "#### Skip Gram\n",
    "\n",
    "Another approach is to create a model such that given the center word \"jumped\", the model will be able to predict or generate the surrounding words `\"The\", \"cat\", \"over\", \"the\", \"puddle\". Here we call the word \"jumped\" the context.` We call this type of model a Skip-Gram model.\n",
    "\n",
    "The setup is largely the same but we essentially swap our x and y i.e. x in the CBOW are\n",
    "now y and vice-versa. The input one hot vector (center word) we will represent with an x (since there is only one). And the output vectors as $y^j$. We define V and U the same as in CBOW\n",
    "\n",
    "<img src='./images/skipgram.PNG' height=100 width=300 align='right'/>\n",
    "\n",
    "\n",
    "\n",
    "We breakdown the way this model works in these 6 steps:\n",
    "\n",
    "1. We generate our one hot input vector $x \\epsilon R^{|V|}$ of the center word.\n",
    "\n",
    "\n",
    "\n",
    "2. We get our embedded word vector for the center word $v_c = Vx\\;\\;\\epsilon \\;R^n$\n",
    "\n",
    "\n",
    "\n",
    "3. Generate a score vector $z = Uv_c$\n",
    "\n",
    "\n",
    "\n",
    "4. Turn the score vector into probabilities,  $\\hat{y} = softmax(z)$\n",
    "\n",
    "\n",
    "\n",
    "5. Note that $\\hat{y}_{c-m} ....\\hat{y}_{c-1} \\hat{y}_{c+1}.....\\hat{y}_{c+m} \\;$are the probabilities of observing each context word.\n",
    "\n",
    "\n",
    "\n",
    "6. We desire our probability vector generated to match the true probabilities which is ${y}^{c-m} ....{y}^{c-1} {y}^{c+1}.....{y}^{c+m} \\;$, the one hot vectors of the actual output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-kansas",
   "metadata": {},
   "source": [
    "The objective function we are trying to minimize assuming independence assumption. \n",
    "In other words, given the center word, all output words are completely independent.\n",
    "\n",
    "\n",
    "<img src=\"images/objective_skipgram.PNG\" width=\"600\" height=\"900\" align='center'> \n",
    "\n",
    "\n",
    "\n",
    "With this objective function, we can compute the gradients with\n",
    "respect to the unknown parameters and at each iteration update\n",
    "them via Stochastic Gradient Descent.\n",
    "\n",
    "Note that\n",
    "\n",
    "$J=-\\sum_{j=0,j\\neq m}^{2m} \\log P(u_{c-m}|V_c)$\n",
    "\n",
    "$J=-\\sum_{j=0,j\\neq m}^{2m} H(\\hat{y}|y_{c-m+j})$\n",
    "\n",
    "\n",
    "where $H(\\hat{y},y_{c-m+j})\\;$is the cross-entropy between the probability\n",
    "vector $\\hat{y}$ and the one-hot vector $y_{c-m+j}$.\n",
    "\n",
    "**Matrix Flow Visualization of algorithm**\n",
    "\n",
    "For illustration purpose, let's assume that the entire corpus is composed of the quote from the Game of Thrones, **\"The man who passes the sentence should swing the sword\"**. There are 10 words (T=10), and 8 unique words (V=8).\n",
    "\n",
    "<img src=\"images/word2vec_skip-gram_matrix.png\" width=\"600\" height=\"900\" align='left'> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-combining",
   "metadata": {},
   "source": [
    "#### Numerical Demonstration of the end to end process\n",
    "\n",
    "Forward Propagation: Computing hidden (projection) layer\n",
    "\n",
    "Center word is \"passes\". Window size is size=1, making \"the\" and \"who\" context words. Hidden layer (h) is looked up from the input weight matrix.\n",
    "\n",
    "**Forward Propogation**\n",
    "\n",
    "<img src=\"images/n1.png\" width=\"500\" height=\"800\" align='center'> \n",
    "\n",
    "<img src=\"images/n2.png\" width=\"500\" height=\"800\" align='center'> \n",
    "\n",
    "\n",
    "\n",
    "**Backward Propogation**\n",
    "<img src=\"images/n3.png\" width=\"500\" height=\"800\" align='center'>\n",
    "\n",
    "\n",
    "**Computing gradient** $\\nabla W_{input}$\n",
    "\n",
    "Gradients of input weight matrix $\\frac{\\partial J}{\\partial W_{input}}$ are computed.\n",
    "\n",
    "Unlike the input weight matrix $W_{input}$, all word vectors in the output weight matrix $W_{output}$ are updated.\n",
    "\n",
    "<img src=\"images/n4.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "<img src=\"images/n5.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "\n",
    "The **updates** to the input and output parameter matrices are done in below.\n",
    "\n",
    "\n",
    "<img src=\"images/n6.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "<img src=\"images/n7.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-bunch",
   "metadata": {},
   "source": [
    "#### Negative Sampling\n",
    "\n",
    "There is an issue with the vanilla Skip-Gram — **softmax is computationally very expensive**, as it requires scanning through the entire output embedding matrix $W_{output}$ to compute the probability distribution of all V words, where V can be millions or more.\n",
    "\n",
    "<img src=\"images/vanilla-skip-gram-complexity.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "Instead we use an alternative called negative sampling with sigmoid function, which rephrases the problem into a set of independent binary classification task of algorithm complexity = O(K+1), where K typically has a range of [5,20].\n",
    "\n",
    "Let's assume that the training corpus has 10,000 unique vocabs (V = 10000) and the hidden layer is 300-dimensional  (N = 300). This means that there are 3,000,000 neurons in the output weight matrix $W_{output}$ that need to be updated for each training sample and for the input weight matrix $W_{input}$, only 300 neurons are updated for each training sample. Since the size of the training corpus (T) is very large, updating 3M neurons for each training sample is unrealistic in terms of computational efficiency. Negative sampling addresses this issue by updating only a small fraction of the output weight neurons for each training sample.\n",
    "\n",
    "In negative sampling, K negative samples are randomly drawn from a noise distribution. K is a hyper-parameter that can be empirically tuned, with a typical range of [5,20]. For each training sample (positive pair: w and cpos), you randomly draw K number of negative samples from a noise distribution Pn(w), and the model will update (K+1)×N neurons in the output weight matrix $W_{output}$. N is the dimension of the hidden layer (h), or the size of a word vector. +1 accounts for a positive sample.\n",
    "\n",
    "With the above assumption, if you set K=9, the model will update (9+1)×300=3000 neurons, which is only 0.1% of the 3M neurons in Woutput. This is computationally much cheaper than the original Skip-Gram, and yet maintains a good quality of word vectors.\n",
    "\n",
    "The below figure has 3-dimensional hidden layer (N=3), 11 vocabs (V=11), and 3 negative samples (K=3).\n",
    "\n",
    "<img src=\"images/neg_vs_skip.png\" width=\"700\" height=\"800\" align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-syndrome",
   "metadata": {},
   "source": [
    "**How does negative sampling work?**\n",
    "\n",
    "With negative sampling, word vectors are no longer learned by predicting context words of a center word. Instead of using softmax to compute the V-dimensional probability distribution of observing an output word given an input word, $p(w_O|w_I)$, the model uses sigmoid function to learn to differentiate the actual context words (positive) from randomly drawn words (negative) from the noise distribution $P_n(w)$.\n",
    "\n",
    "Assume that the center word is \"regression\".\n",
    "\n",
    "It is likely to observe \"regression\" + {\"logistic\", \"machine\", \"sigmoid\", \"supervised\", \"neural\"} \n",
    "\n",
    "pairs, but it is unlikely to observe \"regression\" + {\"zebra\", \"pimples\", \"Gangnam-Style\", \"toothpaste\", \"idiot\"}.\n",
    "\n",
    "\n",
    "The model maximizes the probability p(D=1|w,cpos) of observing positive pairs, while minimizing the probability p(D=1|w,cneg) of observing negative pairs. \n",
    "\n",
    "The idea is that if the model can distinguish between the likely (positive) pairs vs unlikely (negative) pairs, good word vectors will be learned.\n",
    "\n",
    "**Negative sampling converts multi-classification task into binary-classification task.**\n",
    "\n",
    "<img src=\"images/neg_binomial.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "\n",
    "`The new objective is to predict, for any given word-context pair (w,c), whether the word (c) is in the context window of the the center word (w) or not. Since the goal is to identify a given word as True (positive, D=1) or False (negative, D=0)`\n",
    "\n",
    "\n",
    "The probability of a word (c) appearing within the context of the center word (w) can be defined as\n",
    "\n",
    "$p(D=1|w,c;\\theta)=\\frac{1}{1+exp(-\\bar{c}_{output_{(j)}} \\cdot w)}\n",
    "\\in \\mathbb{R}^{1}\n",
    "\\tag{1}$\n",
    "\n",
    "\n",
    "c is the word you want to know whether it came from the context window or the noise distribution. w is the input (center) word, and θ is the weight matrix passed into the model. \n",
    "\n",
    "Note that w is equivalent to the hidden layer h.\n",
    "\n",
    "$\\bar{c}_{output_{(j)}}$ is the word vector from the output weight matrix $W_{output}$\n",
    "\n",
    "The equation ($1$) computes the probability that the given word (c) is a positive word (D=1). It only needs to be applied K+1 times instead of V times for every word in the vocabulary, because $\\bar{c}_{output_{(j)}}$ comes from the concatenation of a true context word $c_{pos}$ and K negative words \n",
    "\n",
    "$\\bar{W}_{neg} = \\{ \\bar{c}_{neg, j}|j=1,\\cdots,K \\}$\n",
    "\n",
    "$\\bar{c}_{output{(j)}} \\in \\{\\bar{c}_{pos}\\} \\cup \\bar{W}_{neg}\n",
    "\\tag{2}$\n",
    "\n",
    "\n",
    "This probability is computed K+1 times to obtain a probability distribution of a true context word and K negative samples:\n",
    "\n",
    "$\\left[ \\begin{array}{c} p(D=1|w,c_{pos}) \\\\ p(D=1|w,c_{neg, 1}) \\\\ p(D=1|w,c_{neg, 2}) \\\\ p(D=1|w,c_{neg, 3}) \\\\ \\vdots \\\\ p(D=1|w,c_{neg, K}) \\end{array} \\right] \n",
    "= \\frac{1}{1+exp(-(\\{\\bar{c}_{pos}\\} \\cup \\bar{W}_{neg}) \\cdot h)}\\in \\mathbb{R}^{K+1}\\tag{3}$",
    "\n",
    "Example of optimizing\n",
    "\n",
    "<img src=\"images/neg_opt_1.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "\n",
    "By the time the output probability distribution is nearly one-hot-encoded as in iter=4 of the above figure, weight matrices θ are optimized and good word vectors are learned. This optimization is achieved by maximizing the dot product of positive pairs $\\bar{c}_{pos}\\cdot \\bar{w}$ and minimizing the dot product of negative pairs $c_{neg}\\cdot w$\n",
    "\n",
    "\n",
    "**Noise Distribution**\n",
    "\n",
    "$P_n(w) = \\left(\\frac{U(w)}{Z}\\right)^{\\alpha}$\n",
    "\n",
    "\n",
    "Imagine a distribution of words based on how many times each word appeared in a corpus, denoted as $U(w)$ (this is called unigram distribution). For each word w, divide the number of times it appeared in a corpus by a normalization factor Z so that the distribution becomes a probability distribution of range [0,1) and sums up to 1. Raise the normalized distribution to the power of α so that the distribution is \"smoothed-out\". Then this becomes your noise distribution $P_n(w)$ — normalized frequency distribution of words raised to the power of α.\n",
    "\n",
    "\n",
    "Raising the unigram distribution U(w) to the power of α has an effect of smoothing out the distribution. It attempts to combat the imbalance between common words and rare words by decreasing the probability of drawing common words, and increasing the probability drawing rare words.\n",
    "\n",
    "*Effect of raising power of unigram distribution U(w)*\n",
    "\n",
    "<img src=\"images/noise_dist.png\" width=\"700\" height=\"800\" align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-eating",
   "metadata": {},
   "source": [
    "#### Numerical Demo\n",
    "\n",
    "Center (input) word is Ned, and window size is C=2, making Stark and is context words. Number of negative samples drawn from the noise distribution for each positive pair is K=3.\n",
    "\n",
    "\n",
    "<img src=\"images/neg_training.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "\n",
    "The center word Ned will be observed in a pair with context words (postive) like Stark, because it is his last name. The same thing goes for is too, because is is a verb tense used to describe a singular object.\n",
    "\n",
    "However, Ned most likely won't be observed in a pair with random words (negative) like pimples, zebra, donkey within the book. If the model can differentiate between positive pairs and negative pairs. good word vectors will be learned.\n",
    "\n",
    "**Forward Propagation: Computing hidden (projection) layer**\n",
    "\n",
    "Hidden layer (h) is looked up from $W_{input}$ by multiplying the one-hot-encoded input vector with the input weight matrix $W_{input}$.\n",
    "\n",
    "<img src=\"images/neg_hidden.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "**Forward Propagation: Sigmoid output layer**\n",
    "\n",
    "Output layer is a probability distribution of positive and negative words $c_{pos} \\cup W_{neg}$\n",
    "\n",
    "given a center word (w). It is computed with sigmoid $\\sigma(x) = \\frac{1}{1+exp(-x)}$\n",
    "\n",
    "<img src=\"images/neg_forward_2.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "**Backward Propagation: Prediction Error**\n",
    "\n",
    "The details about the prediction error is described above. Since our current positive word is `Stark`, $t_j=1$ for `Stark` and $t_j=$0  for other negative words (pimples, zebra, idiot).\n",
    "\n",
    "<img src=\"images/neg_backward_1.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "\n",
    "**Backward Propagation: Computing $\\nabla W_{input}$**\n",
    "\n",
    "Gradients of input weight matrix $\\frac{\\partial J}{\\partial W_{input}}$ are computed.\n",
    "Just like vanilla Skip-Gram, only the word vector in the input weight matrix $W_{input}$ that corresponds to the input (center) word w is updated\n",
    "<img src=\"images/neg_backward_3.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "\n",
    "**Backward Propagation: Computing $\\nabla W_{output}$**\n",
    "\n",
    "With negative sampling, only a fraction of word vectors in the output weight matrix $W_{output}$ is updated. Gradients for K+1 word vectors for positive and negative words in the Woutput are computed\n",
    "<img src=\"images/neg_backward_4.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "**Backward Propagation: Updating Weight matrices**\n",
    "\n",
    "Input and output weight matrices ($[W_{input} W_{output}]$) are updated\n",
    "<img src=\"images/neg_backward_5.png\" width=\"700\" height=\"800\" align='center'>\n",
    "<img src=\"images/neg_backward_6.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-broadway",
   "metadata": {},
   "source": [
    "#### Positive word pair: (Ned, is)\n",
    "\n",
    "\n",
    "The center word Ned has two context words: Stark and is. This means that we have two positive pairs = two updates to make. Since we already update the matrices ($[W_{input} W_{output}]$) using (Ned, Stark), we will use (Ned, is) to update weight matrices this time.\n",
    "\n",
    "In negative sampling, we draw new K negative words for each positive pairs. Assume that we randomly drew coins, donkey, and machine as our negative words this time.\n",
    "\n",
    "**Forward Propagation: Computing hidden (projection) layer**\n",
    "<img src=\"images/neg_hidden_2.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "\n",
    "**Forward Propagation: Sigmoid output layer**\n",
    "<img src=\"images/neg_forward_2_2.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "\n",
    "**Backward Propagation: Prediction Error**\n",
    "Our current positive word is Stark: $t_j=1$ for Stark and $t_j=0$ for other negative words (coins, donkey, and machine)\n",
    "<img src=\"images/neg_backward_1_2.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "\n",
    "**Backward Propagation: Computing $\\nabla W_{input}$**\n",
    "<img src=\"images/neg_backward_2_2.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "\n",
    "**Backward Propagation: Computing $\\nabla W_{output}$**\n",
    "<img src=\"images/neg_backward_3_2.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "\n",
    "**Backward Propagation: Updating Weight matrices**\n",
    "<img src=\"images/neg_backward_4_2.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "\n",
    "<img src=\"images/neg_backward_5_2.png\" width=\"700\" height=\"800\" align='center'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-communist",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf\n",
    "https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
